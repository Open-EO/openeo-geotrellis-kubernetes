ARG OPENEO_BASE_IMAGE=vito-docker.artifactory.vgt.vito.be/openeo-base:latest
FROM ${OPENEO_BASE_IMAGE}

# Build arguments
ARG JMX_PROMETHEUS_JAVAAGENT_VERSION=0.13.0
ARG HADOOP_VERSION=3.4.1
ARG SPARK_VERSION=3.5.3
ARG SCALA_VERSION=2.12

USER root

# Copy entrypoints and helper scripts
COPY docker/AlmaLinux/entrypoint.sh /opt/entrypoint.sh
COPY docker/AlmaLinux/decom.sh /opt/decom.sh
COPY --chmod=700 docker/AlmaLinux/addlocalnodeip /bin/addlocalnodeip

# Add S3 dependencies
ADD https://artifactory.vgt.vito.be/artifactory/libs-release/org/apache/hadoop/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar $SPARK_HOME/jars
#ADD https://artifactory.vgt.vito.be/artifactory/libs-release/com/amazonaws/aws-java-sdk-bundle/1.12.263/aws-java-sdk-bundle-1.12.263.jar $SPARK_HOME/jars

# Conditionally override jars provided by Spark itself,
# to fix https://github.com/Open-EO/openeo-geopyspark-driver/issues/1144
#
# Spark sometimes ships with Hadoop 3.3.4 client jars by default.
# If those jars are found, we remove them and replace them
# with the matching Hadoop ${HADOOP_VERSION} jars.
RUN if [ -f "$SPARK_HOME/jars/hadoop-client-api-3.3.4.jar" ]; then \
      echo "Found Spark 3.3.4 Hadoop client jars — removing them..." && \
      rm -f \
        "$SPARK_HOME/jars/hadoop-client-api-3.3.4.jar" \
        "$SPARK_HOME/jars/hadoop-client-runtime-3.3.4.jar" && \
      echo "Downloading replacement jars for Hadoop ${HADOOP_VERSION}..." && \
      curl -fsSL -o "$SPARK_HOME/jars/hadoop-client-api-${HADOOP_VERSION}.jar" \
        "https://artifactory.vgt.vito.be/artifactory/libs-release/org/apache/hadoop/hadoop-client-api/${HADOOP_VERSION}/hadoop-client-api-${HADOOP_VERSION}.jar" && \
      curl -fsSL -o "$SPARK_HOME/jars/hadoop-client-runtime-${HADOOP_VERSION}.jar" \
        "https://artifactory.vgt.vito.be/artifactory/libs-release/org/apache/hadoop/hadoop-client-runtime/${HADOOP_VERSION}/hadoop-client-runtime-${HADOOP_VERSION}.jar" ; \
    else \
      echo "No Spark 3.3.4 Hadoop client jars found — skipping override." ; \
    fi

# Add Spark Hadoop cloud support
ADD https://artifactory.vgt.vito.be/artifactory/libs-release/org/apache/spark/spark-hadoop-cloud_${SCALA_VERSION}/${SPARK_VERSION}/spark-hadoop-cloud_${SCALA_VERSION}-${SPARK_VERSION}.jar $SPARK_HOME/jars

# Create spark user and set permissions, install Python packages, and other setup
RUN adduser -u 18585 -d /opt/spark/work-dir spark && \
    chown 18585:18585 /opt/spark/work-dir && \
    printf "[global]\nextra-index-url = https://artifactory.vgt.vito.be/artifactory/api/pypi/python-openeo/simple\n" > /etc/pip.conf && \
    curl -O https://artifactory.vgt.vito.be/artifactory/libs-release/io/prometheus/jmx/jmx_prometheus_javaagent/${JMX_PROMETHEUS_JAVAAGENT_VERSION}/jmx_prometheus_javaagent-${JMX_PROMETHEUS_JAVAAGENT_VERSION}.jar && \
    chmod +x /opt/entrypoint.sh /opt/decom.sh && \
    PYTHONPLATLIBDIR=lib64 /opt/venv/bin/python3 -m pip install -I --upgrade pip && \
    chown -R 18585:18585 $SPARK_HOME/jars && \
    PYTHONPLATLIBDIR=lib64 /opt/venv/bin/python3 -m pip install py4j boto3==1.35.99 kubernetes==12.0.1 PyYAML==5.3.1 Jinja2==3.1.4 spark_memlogger==0.6 && \
    rm -rf /root/.cache && \
    yum clean all && \
    rm -rf /var/cache/yum/* && \
    mkdir /opt/tmp /batch_jobs /spark_tmp && \
    touch /opt/openeo_python.log && \
    ln -s /opt/venv /opt/openeo && \
    chown -R 18585:18585 /opt/openeo /opt/tmp /batch_jobs /spark_tmp /opt/openeo_python.log && \
    echo "spark   ALL=(root) NOPASSWD:SETENV: /bin/addlocalnodeip" > /etc/sudoers.d/spark

# install openjpeg here, to avoid a conflict with gdal. Better option is to clean up dependency chain downstream
# this openjpeg contains important bugfix for reading JPEG2000 files
RUN [[ ${OPENEO_BASE_IMAGE} == *python311* ]] && rpm -i --nodeps --force https://artifactory.vgt.vito.be/artifactory/vito-yum-almalinux9-public/openjpeg-2.5.3-2.el9.x86_64.rpm


# Set up custom openEO processes
COPY openeo-geopyspark-k8s-custom-processes /opt/openeo-geopyspark-k8s-custom-processes
ENV OPENEO_CUSTOM_PROCESSES=/opt/openeo-geopyspark-k8s-custom-processes/src/openeo_geopyspark_k8s_custom_processes/custom_processes.py

# Fix encoding for gdalinfo stdout
ENV LANG=en_US.UTF-8

WORKDIR /opt/spark/work-dir

ENTRYPOINT [ "/opt/entrypoint.sh" ]

USER 18585
